CS 111, Lab 4. Answers to Questions, Name: Srikanth Ashtalakshmi V Rajendran, UID: 804478782

Part 1 - parallel updates to a shared variable

QUESTIONS 1.1

To consistently get an error in the counter's final result, it takes about 1500 iterations when the no. of threads are
restricted to 2, and it takes around 2000 threads when the no. of iterations are resitricted to 10.

1. It takes this many iterations because when there are more iterations, each thread is more likely to last longer, increasing
   the chances that the threads execute in parallel. The threads are created sequentially, so the no. of iterations have to be
   large so that the first thread does not complete before the second, as this eliminates parallelism and chances of race
   conditions occuring. On the other hand, a large number of threads also increases the chances of two or more threads interfering
   with each other, causing a race condition.

2. When there are a small no. of iterations, an earlier thread completes the task before the next thread is created and starts
   executing. Hence, when there are a small no. of iterations, most threads don't temporally overlap, decreasing the chances
   for race conditions to occur.

QUESTIONS 1.2

1. The execution time also takes into account the overhead, such as creating and joining the threads. Therefore, when the no.
   of iterations is large, the overhead time is more finely divided among each operation, resulting in a smaller measurement
   of time per operation.

2. To obtain the correct cost, the time per operation for a large number of iterations and threads can be obtained. We know this
   value is correct because the time per operation eventually reaches a limit, when the overhead is very finely divided among
   each operation.

3. The yield runs are slower due to context switch when the current thread gives up control over system resources to another
   thread. This frequent switching between threads contributes to extra execution time.

4. We can obtain valid timings indirectly by estimating the time context switches take and subtracting this value from the final
   result. 

QUESTIONS 1.3

1. When there is a low number of threads, there is not much competition to access a shared resource (the lock in this case).
   Hence, all three synchronization options have a small influence on the timing characteristics of the program.

2. Synchronization is needed when several threads try to simultaneously run in a critial section. When there are a large number
   of threads, there is more parallelism and there is greater demand for the locks. Therefore, the performance of each
   synchronization option is more distinctly observable as the number of threads grows. 

3. Spinlocks are expensive for a large number of threads because in this method of synchronization threads are taking up system
   resources even as they wait (while spinning in a while loop). Hence the 'spinning' of a large number of threads increases
   the execution time. Conversely, pthread_mutex locks are not very expensive as instead of 'spinning' when waiting for a lock,
   the corresponding thread is blocked or put to sleep until it is granted the lock.


Part 2 - parallel updates to complex data structures

QUESTIONS 2.1

1. As in part 1, the overhead cost, such as creating and joining threads, is more finely distributed among each operation when
   the number of operations is larger. This timing inconsistency can be corrected by measuring time per operation after running
   a large number of operations (i.e. greater no. of threads and/or iterations). The overhead cost will eventually get diluted
   and the time per operation reaches a limit. This limiting value is taken as the corrected measurement.

QUESTIONS 2.2

2. The time per operation is much greater in part 2 as not only are the operations performed significantly more complex, but also
   because each iteration of a thread requires a lock to be obtained twice (for insertion and lookup/delete). In part 2, the
   length of the linked list is another factor that affects performance, whereas in part 1, the counter value does not affect
   the performance of subsequent additions. Hence, for the same no. of operations, part 2 will have a larger execution time than
   part 1.

QUESTIONS 2.3

1. With a larger number of threads per list, the cost of synchronization increases as locks will now be more frequently used. If
   a list is operated on by only a few threads then not much synchronization is needed. Therefore, a greater no. of threads per
   list leads to a larger execution time.

2. Threads per list is more interesting as when threads operate on separate lists, the threads are independent of each other's
   execution. Knowing the number of threads that are concurrently editing a single list is more useful as we're interested in
   the interdependency of threads and their synchronization.

Part 3 - sleep/wakeup races

QUESTIONS 3.1

1. The mutex must be held so that the thread is not inturrupted by a signal while still processing. Moreover, if the mutex is not
   held, multiple threads can end up waiting on each other leading to a deadlock.

2. The mutex must be released so that a signaling function (pthread_cond_signal) can inform when the thread must wake up and
   then the thread can actually acquire the lock and proceed.

3. As the thread is now woken up, it must hold onto the lock so that other threads do not simultaneously process the critical
   section along with the current thread.

4. This is done to ensure that both these instructions are atomically executed. If the mutex was released before calling
   pthread_cond_wait, then some other thread could end up locking the mutex, causing the pthread_cond_wait to undergo undefined
   behaviour.

5. As a user program, there is no guarantee of atomicity and continuity (i.e. no interrupts). However, if this program is
   implemented by a system call, we can control concurrency and interruption, to ensure that undefined behavior or a deadlock
   doesn't take place.
   

